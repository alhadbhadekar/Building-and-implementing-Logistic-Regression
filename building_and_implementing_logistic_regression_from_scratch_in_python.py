# -*- coding: utf-8 -*-
"""Building and implementing Logistic Regression from scratch in Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HQohGiV2x55lc8n9Ive9GqqoVIky2lQg

**Logistic Regression:**

![Picture4.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAkMAAABLCAMAAABEK3PtAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAzUExURQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKMFRskAAAAQdFJOUwATIi88SlhndoWWqLjL3O5QTd6UAAAACXBIWXMAABcRAAAXEQHKJvM/AAAEFklEQVR4Xu3ci3KjIBQGYEGDiAi8/9Pu4ZJNE3Cjkm3F/t/MjiaTpo3+Hi6S7QAAAAAAAAAAAAAAAOATuNJpD+AIppxDhuA4Ju1skCGocJt5J5EhqIQMQS1kCGohQ1ALGYJayBDUQoagFjIEtZAhqIUMQS1kCGohQ1CpN84NaR9gP3eHGF2dcgtLuwBHjG5GpwVqCKfCP4CD+tCQKSfSYwCAb3YfMzHr+rADsJNK3WnMBcJRPBYiKkOYw4GDVBiSjStlSKZJwkSmpwG+oELEu86slCFkCDbwhUigNwQVfCFa9vaGUl36pPTO0CJ14F5HOu+flN4ZWkSFCIMyqKPckvYy6FP/OH8Kzj/9+4/5RWQoGtIBIPUlW/u3oa3fbLjXLZxNeyeGOeq3pFs4bZh1c3yiinHWv9uNIrRh4dbwkd/5n/1ohqy/Gp2z514FN8e/Tzvzib9TxM7DLQ+HLJS50Y1p78R+MkM8tI/a3eLDs7JhaQy17B8ZfDDnKIrM5MtHSxlSLdwNP5whJqtbauF/dSOrl/qPdQhn/4F1aNCelTJkGugO+cWwaW8fSe1Q2q0ytrGKkm39fwF9B9xvWWiyqHhlIRB0xKdSeSlkiNFrb4szV1yaI8xCA4z04Cu9M1j+gLZAxZ7wQxhfPTzO/hjaKp8l/2gqRM86Uay9hQzd3KhGeq+T9xiP4Hq4H6QXbzL0mC6IDUNv2/hWCY2iXjptqxlKK/umeHhkIS1qpVksZIiGhL5PvZy9y3jQoQy9YLaNC4zbHS1uXI1lwpKIbjbhuSfjypxPIUM6ljF90cm5HRkSdNGaQvVmi22joV/2fAlP+ygIO/kNL5ShnnqSzx/b96G++NL6pQL36zPEtBv9sCY/C2GE0i2nr9PUAu/IejjhWoaqMuU3kpiZX8cRqxnq042oqy403ZyhJSQlXJ3PlJ8/Y6oQrnOhT7pnok9ShoRlkn6IF869XhgvN2Z5Wza6yW946QK8gq0ZknHkZbJL+X71FXoMp1Ie1q/3qSlDjFpuQVHSIQJPlG++y33kPENzfN1lVwm+ZGjtBm363lHDh2FOI2v59AnWMyTcLKkJGpwc85sjcVRfnpTLM5RWKa8tVm7exgwJqjP8ppxutRqLNKzvY7vy3uCMpXPuN6na0LGJO/RkCA8du8LhyDKUvndzdBr4/Da2ZZNPlJ6avZBoFBWyM9itczR0YMJPpA35myF6s3Ag6CWFgVaWIX9vv6OOVbPX3zsbM9T6sHQOZTXYeibT/f3HfMA9Qzca1fve32Do3fJBf5ahUQv69Uv+yqv4JRn6Rnl/6Oo2ZsiPdEl/0ZEFVOBUZQsVJssQt4Z3XJamqeFX+zuyfY1RlqGup7CZKV8tA1CWZwgAAAAAAAAAAAAAAGC7rvsDkSM8e6OA8XgAAAAASUVORK5CYII=)

Y_hat --> predicted value

X --> Input Variable

w --> weight

b --> bias

Y_hat --> predicted value

X --> Input Variable

w --> weight

b --> bias

Gradient Descent:

Gradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.

w = w - α*dw

b = b - α*db

Learning Rate:

Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.

Derivatives:

![Picture3.png](data:image/png;base64,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)

Importing the Dependencies
"""

# importing numpy library
import numpy as np

"""Logistic Regression"""

class Logistic_Regression():

  # declaring learning rate & number of iterations (Hyperparametes)
  def __init__(self, learning_rate, no_of_iterations):

    self.learning_rate = learning_rate
    self.no_of_iterations = no_of_iterations

  # fit function to train the model with dataset
  def fit(self, X, Y):

    # number of data points in the dataset (number of rows) --> m
    # number of input features in the dataset (number of columns) --> n
    self.m, self.n = X.shape

    #initiating weight & bias value
    self.w = np.zeros(self.n)
    self.b = 0
    self.X = X
    self.Y = Y

    # implementing Gradient Descent for Optimization
    for i in range(self.no_of_iterations):
      self.update_weights()

  # Sigmoid Equation & Decision Boundary

  def update_weights(self):
    # Y_hat formula (sigmoid function)

    Y_hat = 1 / (1 + np.exp( - (self.X.dot(self.w) + self.b ) ))

    # derivatives

    dw = (1/self.m)*np.dot(self.X.T, (Y_hat - self.Y))
    db = (1/self.m)*np.sum(Y_hat - self.Y)

    # updating the weights & bias using gradient descent

    self.w = self.w - self.learning_rate * dw
    self.b = self.b - self.learning_rate * db


  # Sigmoid Equation & Decision Boundary
  def predict(self, X):
    Y_pred = 1 / (1 + np.exp( - (X.dot(self.w) + self.b ) ))
    Y_pred = np.where( Y_pred > 0.5, 1, 0)
    return Y_pred

"""Implementing Above Logistic Regression Model

Importing Dependencies
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

"""Data Collection and Analysis

PIMA Diabetes Dataset
"""

# loading the diabetes dataset to a pandas DataFrame
diabetes_dataset = pd.read_csv('/content/diabetes.csv')

# printing the first 5 rows of the dataset
diabetes_dataset.head()

# number of rows and Columns in this dataset
diabetes_dataset.shape

# getting the statistical measures of the data
diabetes_dataset.describe()

diabetes_dataset['Outcome'].value_counts()

"""0 --> Non-Diabetic

1 --> Diabetic
"""

# separating the data and labels
features = diabetes_dataset.drop(columns = 'Outcome', axis=1)
target = diabetes_dataset['Outcome']

print(features)

"""Data Standardization"""

scaler = StandardScaler()

scaler.fit(features)

standardized_data = scaler.transform(features)

print(standardized_data)

features = standardized_data
target = diabetes_dataset['Outcome']

print(features)
print(target)

"""Train Test Split"""

X_train, X_test, Y_train, Y_test = train_test_split(features,target, test_size = 0.2, random_state=2)

print(features.shape, X_train.shape, X_test.shape)

"""Training the Model"""

classifier = Logistic_Regression(learning_rate=0.01, no_of_iterations=1000)

#training the support vector Machine Classifier
classifier.fit(X_train, Y_train)

"""Model Evaluation

Accuracy Score
"""

# accuracy score on the training data
X_train_prediction = classifier.predict(X_train)
training_data_accuracy = accuracy_score( Y_train, X_train_prediction)

print('Accuracy score of the training data : ', training_data_accuracy)

# accuracy score on the test data
X_test_prediction = classifier.predict(X_test)
test_data_accuracy = accuracy_score( Y_test, X_test_prediction)

print('Accuracy score of the test data : ', test_data_accuracy)

"""Making a Predictive System"""

input_data = (5,166,72,19,175,25.8,0.587,51)

# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = classifier.predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('The person is not diabetic')
else:
  print('The person is diabetic')

